---
layout: post
title:  "CVPR2018 Summary"
date:   2018-12-21
excerpt: "Just about everything you'll need to style in the theme: headings, paragraphs, blockquotes, tables, code blocks, and more."
tag:
- CVPR2018 
- GAN
- Face Detection
- Face Age Progression
- Face Anti-Spoofing
- Face Alignment
- Face Synthesis
- Face Super-Resolving
comments: true
---

## Finding Tiny Faces in the Wild with Generative Adversarial Network[^1]

该论文提出的方法主要解决的问题是图像中小人脸的检测问题。针对检测模型提取出的Proposal，该论文基于 GAN 的方法将其中分辨率小且模糊的人脸进行超分辨率及清晰化处理。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="http://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">论文提出的模型流程图：(A) 送入网络的输入图像；(B)  MB-FCN 检测器，可裁切出输入图像中的人脸区域（正例图像）和非人脸区域（反例图像），这些数据用来送入后面的生成器和判别器用于模型训练；(C) 使用 MB-FCN 检测器生成的人脸区域数据（正例图像）和非人脸区域数据（反例图像）；(D) 提出的生成器网络结构，用于从一个低分辨率的图像重构出一个清晰的超分辨率图像（分辨率提高4倍），生成器网络包括上采样子网络和精调子网络；(D) 使用 VGG19 结构作为判别器，后面接有两个并行的 FC 层，分别用来判别输入的数据是否是真实的超分辨率图像和是否是人脸区域。</div>
</center>

#### GAN

常规的 GAN 的目标函数定义如下：

$$ \mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x\sim p_{data}(x)}[log D_{\theta}(x)] + \mathbb{E}_{z\sim p_{z}(z)}[log(1 - D_{\theta}(G_{\omega}(z)))] $$

其中，$$z$$ 表示随机噪声，$$x$$ 是真实的数据，$$\theta$$ 和 $$\omega$$ 分别是生成器 $$G$$ 和判别器 $$D$$ 的参数。模型优化的目标如下：

$$ \arg \min_{G}\max_{D} \mathcal{L}_{GAN}(G, D) $$

和常规的 GAN 的类似，除了输入的不是随机噪声，而是正例和反例图像，目标函数和优化目标如下：

$$ \arg \min_{\omega_{G}}\max_{\theta_{D}} \mathbb{E}_{(I^{HR},y)\sim p^{train}(I^{HR}, y)}[\log D_{\theta_{D}}(I^{HR}, y)] + \\
\mathbb{E}_{(I^{LR},y)\sim p^{G}(I^{LR}, y)}[\log (1 - D_{\theta_{D}}(G_{\omega_{G}}(I^{HR}, y)))] $$

其中，$$I^{LR}$$ 表示的是低分辨率的图像，$$I^{HR}$$ 是高分辨率的图像，$$y$$ 是输入图像的标签。在判别网络，判别器不仅要判别生成图像的高分辨率图像和真实的高分辨率图像，还要判别人脸图像和非人脸图像。

#### 损失函数

论文中采用了两种损失函数，分别是 pixel-wise loss 和 adversarial loss。


**pixel-wise loss.** 通过引入 pixel-wise MSE loss，可以让生成器生成的高分辨率图像和真实高分辨率图像在纹理信息上接近。损失函数如下：

$$ L_{MSE}(\omega) = \frac{1}{N} \sum_{i=1}^{N}(\| G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2} + \| G_{2_{\omega 2}}(G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2}) $$

其中，$$ I^{LR} $$ 和 $$ I^{HR} $$ 分别表示低分辨率模糊图像和高分辨率图像，$$ G_{1} $$ 代表了生成器网络中的上采样子网络，而 $$G_{2}$$ 代表了生成器网络中的精调子网络，$$\omega$$ 是生成器的参数。但 MSE 优化问题总是会丢失图像的高频信息，造成生成的图像偏向于模糊或者过平滑。

**Adversarial loss.** 为了得到更加真实清晰的细节信息，作者引入了 adversarial loss，定义如下：

$$ L_{adv} = \frac{1}{N}\sum_{i=1}^{N}\log (1 - D_{\theta}(G_{\omega}(I_{i}^{LR}))) $$

引入的 adversarial loss 使得生成器生成出来的图像更加的清晰，包含有更多的高频信息以可以欺骗判别器模型。

**Classification loss.** 训练数据除了有低分辨率和高分辨率之分，还有人脸和非人脸的差异，由此引入了 classification loss，用来判别是否是人脸图像。损失函数定义如下：

$$ L_{cls} = \frac{1}{N}\sum_{i=1}^{N}(\log (y_{i} - D_{\theta}(G_{\omega}(I_{i}^{LR}))) + \log (y_{i} - D_{\theta}(I_{i}^{HR}))) $$

其中 $$y_{i}$$ 是图像是否是人脸图像的标签，$$y_{i}=1$$ 表示图像是人脸图像，$$y_{i}=0$$ 表示的是非人脸图像。

**Objective funciton.** 目标函数是上述三个损失函数的加权求和：

$$ \max_{\theta}\min_{\omega} \frac{1}{N}\sum_{i=1}^{N}\alpha(\log (1 - D_{\theta}(G_{\omega}(I_{i}^{LR})) +  \log D_{\theta}(I_{i}^{HR})) + \\ 
(\| G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2} + \| G_{2_{\omega 2}}(G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2}) + \\ 
\beta(\log (y_{i} - D_{\theta}(G_{\omega}(I_{i}^{LR}))) + \log (y_{i} - D_{\theta}(I_{i}^{HR})))$$

其中，$$\alpha$$ 和 $$\beta$$ 分别代表权重。

### 消融实验

实验用的数据集是 WIDER FACE dataset，总共包含 32,203 张密集的人脸图像，其中按照人脸的大小，分为简单/中等/难三个子数据集。论文中作者做了消融实验来验证提出的一系列损失函数的有效性，如下图所示：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="http://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">消融实验：Baseline 方法和分别加入精调网络、对抗损失、分类损失和论文提出的方法在 WIDER FACE dataset 上的表现。</div>
</center>

 
## Learning Face Age Progression: A Pyramid Architecture of GANs[^2]

该论文提出了一个基于 GAN 的做年龄老化的方法，同时能够保持人的身份信息不改变。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_3.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">提出的年龄老化框架图。一个基于 CNN 的生成器 G 用于做年龄转换，欧式空间下的平方损失用来约束生成的图像。GAN 损失在图像的纹理信息到高层的语义信息上来判别生成的老化人脸和真实的老年人脸。身份保持损失在人脸特征空间约束人脸的身份信息。</div>
</center>

#### 判别器

和以往判别器采用对数似然作为目标函数不同，论文中使用最小均方损失代替对数似然。作为判别器的输入，正例为真实的年老人脸图像，而反例有两类，一个是真实的年轻人脸图像，另外一个是有生成器生成的年老人脸图像。论文中 GAN 的损失函数定义如下：

$$ \mathcal{L}_{GAN\_D} = \frac{1}{2}\mathbb{E}_{x\sim P_{old}(x)}[(D_{\omega}(\phi_{age}(x))-1)^{2}] + \\
\frac{1}{2}\mathbb{E}_{x\sim P_{young}(x)}[D_{\omega}(\phi_{age}(G(x)))^{2} + D_{\omega}(\phi_{age}(x))^{2}] $$


$$ \mathcal{L}_{GAN\_G} = \mathbb{E}_{x\sim P_{young}(x)}[D_{\omega}(\phi_{age}(G(x)) - 1)^{2}] $$

介于生成器 G 和判别器 D 之间有一个函数 $$\phi_{age}$$ 用来提取和年龄相关的特征。作者认为在不同年龄群体中的人脸会有一些共性和相似的纹理特征，所以提出了独立于判别器的特征提取函数 $$\phi_{age}$$ 用来提取和年龄相关的特征依次来更好的区分真实的年老人脸图像和生成的年老人脸图像。作者针对 $$\phi_{age}$$ 做了特殊的设计，首先使用 VGG-16 结构训练年龄分类任务的模型作为预训练，去除全连接层后，从输入图像的像素级别一直到高层的年龄相关的语义信息层来提取年龄相关的特征，即采用了一个金字塔的结构。

#### 身份保持

年龄老化之后的图像不应该改变人的身份信息，为此作者引入了年龄保持损失。用一个在大规模人脸数据集上训练做人脸分类任务的模型作为预训练模型，然后去除最后的分类层，直接用一个 triplet-loss 来做度量学习，在特征空间上约束人脸的特征距离。目标损失函数定义如下：

$$ \mathcal{L}_{identity} = \mathbb{E}_{x\sim P_{young}(x)}d(\phi_{id}(x), \phi_{id}(G(x))) $$

其中，$$\phi_{id}$$ 是预训练的人脸身份特征提取模型，$$d$$ 是人脸空间的平方欧式距离。

#### 目标函数

除了判别器损失和人脸保持损失i，作者又引入了一个 pixel-wise L2 损失函数来约束图像空间中生成的年老人脸图像和真实的年老人脸图像的距离，定义如下：

$$ \mathcal{L}_{pixel} = \frac{1}{W \times H \times C}\|G(x)-x\|_{2}^{2} $$

其中，$$x$$ 是输入的人脸图像，$$W$$、$$H$$ 和 $$C$$ 是图像的形状。

最终整个模型的训练损失如下：

$$ \mathcal{L}_{G} = \lambda_{\alpha}\mathcal{L}_{GAN\_G} + \lambda_{p}\mathcal{L}_{pixel} + \lambda_{i}\mathcal{L}_{identity} $$

$$ \mathcal{L}_{D} = \mathcal{L}_{GAN\_D} $$

### 实验

论文在两个数据集 MORPH 和 CACD 上做了大量的实验，包括人脸老化的定性分析和定量分析以及年龄老化后对人脸识别的影响。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_4.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">年龄老化的定性实验，前两行是在 CACD 数据集上的结果，后两行是在 MORPH 数据集上的结果。每组第一张图像是模型输入的年轻人脸图像，紧跟其后的三个人脸图像是做的老化到不同年龄阶段的效果。</div>
</center>

## Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs[^3]

论文提出了通过 GAN 方法来对小的人脸图像做超分辨率处理，进而来提高低分辨率下的人脸关键点定位的准确度。作者提出的模型成为 Super-FAN，能够同时进行好分辨率处理和人脸关键点定位任务。该模型包含了一个基于 GAN 的人脸超分辨率网络和一个通过热度图回归（heatmap regression）的人脸关键点定位的子网络。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_5.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Super-FAN 网络框架，包含三个模块：第一个是作者提出的人脸超分辨率生成网络，第二个是基于 WGAN 的判别器，用来判别原始的高分辨率人脸图像和生成的高分辨率人脸图像，第三个是人脸对齐网络，用来做生成的高分辨率人脸图像的关键点点定位任务并通过热图损失（heatmap loss）来提高超分辨的效果。</div>
</center>

#### 超分辨率网络（Super-resolution network）

作者提出了一个新的基于残差（residual-based）的网络结构用于超分辨率处理，网络的输入和输出的尺寸分别是 $$16\times 16$$ 和 $$64\times 64$$。类比于 SR-ResNet 包含有 15-1-1 的块结构，作者做了改进，提出了一个 12-3-2 的块结构，做出这个改进的依据是模型的最终目的是做超分辨处理，模型最后单个块（Block）不能充分生成清晰的细节信息。同时，作者去除了 SR-ResNet 中的一个长连接（long skip connection）。作者提出的生成器和 SR-ResNet 的网络结构图如下图所示：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_6.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">作者提出的基于残差的生成器和 SR-ResNet 模型对比</div>
</center>

##### 像素和感知损失（Pixel and perceptual losses）

**Pixel loss.** 作者使用像素级别的平均绝对误差（pixel-wise MSE loss）来最小化生成的高分辨率人脸图像和对应的真实高分辨率人脸图像之间的差异。定义如下：

$$ l_{pixel} = \frac{1}{r^{2}WH}\sum_{x=1}^{rW}\sum_{y=1}^{rH}(I_{x,y}^{HR}-G_{\theta_{G}}(I^{LR})_{x,y})^{2} $$

其中，$$W$$ 和 $$H$$ 表示低分辨率人脸图像 $$I^{LR}$$ 的宽和高，$$r$$ 是超分辨处理的上采样倍数。

**Perceptual loss.** 像素级别的平均绝对误差能够达到很高的 PSNR 评价指标，但也会缺失人脸图像的细节，真实度也很低。由此作者引入了感知损失，在 ResNet50（作者使用ResNet50作为生成器模型）的 B1、B2 和 B3 blocks 加入了感知损失，对于第 $$i$$ 层的特征的感知损失定义如下：

$$ l_{feature/i} = \frac{1}{W_{i}H_{i}}\sum_{x=1}^{W_{i}}\sum_{y=1}^{H_{i}}(\phi_{i}(I^{HR}))_{x, y} - \phi_{i}(G_{\theta_{G}}(I^{LR}))_{x,y})^{2} $$

其中，$$\phi_{i}$$ 表示第 $$i$$ 个块的最后一个的卷积层的特征图，$$W_{i}$$ 和 $$H_{i}$$ 是其尺寸。


#### 对抗网路（Adversarial network）

作者使用 WGAN 中的损失函数，定义如下：

$$ l_{WGAN} = \underset{\hat I\sim \mathbb{P}_{g}}{\mathbb{E}}[D(\hat I)] - \underset{I\sim \mathbb{P}_{r}}{\mathbb{E}}[D(I^{HR})] + \lambda \underset{\hat I\sim \mathbb{P}_{\hat I}}{\mathbb{E}}[(\|\nabla_{\hat I}D(\hat I) \|_{2}-1)^{2}] $$

#### 人脸对齐网络（Face Alignment Network）

前面提出的像素、感知和对抗损失只能恢复出清晰的人脸纹理信息，但并不能保证生成的高分辨率人脸的结构和低分辨率的人脸结构一致。为了解决这个问题，作者提出了一个通过热度图回归来做人脸关键点定位的网络，将这个网络和超分辨率处理模型进行整合，提出了一个热度图损失（heatmap loss）来约束生成的高分辨率人脸的结构信息。

论文中指出只要保证生成的高分辨率人脸图像的关键点热图（heatamp）和对应的真实高分辨率人脸图的关键点热图保持一致即可。所以作者并没有直接做人脸关键点定位操作，而是一个了两个 FAN 模型，一个和超分辨处理模块整合为一个端到端的模型，用于提取生成的高分辨率人脸图像的关键点热图，另外一个 FAN 在人脸关键点定位任务上做预训练，预训练的模型用来提取真实的高分辨率人脸图像中的关键点的热图。然后让这前一组热图尽可能的后一组热图保持一致。热图损失（heatmap loss）定义如下：

$$ l_{heatmap} = \frac{1}{N} \sum_{i=1}^{N}\sum_{ij} (\widetilde{M}_{i,j}^{n}-\widehat{M}_{i,j}^{n})^{2} $$

其中，$$ \widetilde{M}_{i,j}^{n}$$ 是由整合进超分辨率处理模块的 FAN 模块产生的合成的高分辨率人脸图像 $$\hat{I}_{HR}$$ 的第 $$n$$ 个关键点在像素坐标 $$(i, j)$$ 处的热图，$$\widehat{M}_{i,j}^{n}$$ 是有另外一个 FAN 生成的真实的高分辨率人脸图像 $$I_{HR}$$ 的热图。

#### 整个的训练损失（Overall training loss）

Super-FAN 的整个的训练损失定义如下：

$$ l^{SR} = \alpha l_{pixel} + \beta l_{feature} + \gamma l_{heatmap} + \zeta l_{WGAN} $$

其中，$$\alpha$$，$$\beta$$，$$\gamma$$ 和 $$\zeta$$ 是对应的权重。

### 实验

作者分别做了人脸超分辨率生成实验和针对生成的人脸图像做的人脸关键点定位实验。在人脸超分辨率生成实验中有定性和定量两种分析，定性的实验结果如下图所示，实验结果度量采用的是 PSNR 和 SSIM：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_7.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">在 LS3D-W balanced dataset 上基于 PSNR 和 SSIM 做的人脸超分辨率定性实验</div>
</center>

在人脸关键点定位实验中，作者首先在一个高分辨率的人脸数据集上训练人脸关键点定位模型 FAN，然后用这个模型来对生成的人脸高分辨率图像做关键点定位，定位的精度即作为人脸超分辨结果的精度。作者对人脸划分了不同的角度范围，实验结果如下：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_8.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">在 LS3D-W balanced test set 上的 AUC 结果</div>
</center>


## Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision[^4]

和以往人脸欺骗检测方法对输入人脸图像直接做二分类不同的是，作者利用了辅助的监督信号来指导人脸欺骗检测模型学习过程，提出了一个基于 CNN-RNN 模型来估计人脸像素级别的深度数据和估计 rPPG 信号，然后估计出的人脸深度和 rPPG 融合后来判别人脸图像是真实人脸还是欺骗人脸。


### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_9.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">基于 CNN-RNN 的人脸欺骗检测方法</div>
</center>

CNN 模块用人脸深度图作为监督信号来发掘真实人脸图像和人脸欺骗图像中细微的纹理特性，然后估计出的人脸深度图像和特征图送入一个非刚性注册层来生成对齐的特征图。RNN 模块用生成的对齐特征图作为输入，rPPG 信号作为监督，来获取视频帧中的时序变化。

#### 深度图像监督（Depth  Map Supervision）

真实人脸图像和欺骗人脸图像在深度图像上有本质的差异（真实人脸图像的深度范围在0到1，而欺骗人脸图像的深度值是0）。如何来获取人脸深度监督信息，论文中使用人脸三维重构的方法先重构出人脸的三维点云，然后通过人脸三维点云映射到二维的深度图像上，最终得到了真实人脸对应的尺寸为 $$32\times 32$$ 的人脸深度图像。

#### rPPG 监督信号（rPPG Supervision）

rPPG 信号提供了人脸纹理变化的时序信息，主要捕获血液流到在脸部产生的亮度变化。作者认为同一个人的心率变化是固定的，所以选择一个人在可控环境下的一段视频提取出的 rPPG 信号作为这个人其他 PIE 条件下的 rPPG 监督监督信号。对于欺骗人脸数据，作者在实验中将其 rPPG 信号设置为0。

#### 网络结构

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_10.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">论文突出的基于 CNN-RNN 的网络结构。每一层网络的卷积核的数量标在了每一层的上面，所有的卷积和的大小为3x3，步长为1，池化层的步长为2。颜色表示：橘色=卷积层，绿色=池化层，紫色=响应图。</div>
</center>

##### CNN 网络

CNN 网络设计为全卷积网络，网络的输入图像的大小为$$256\times 256$$，输出大小为$$32\times 32$$，输出有两个分支，一个是估计出的人脸深度图，另外一个是单通道的特征图。
监督损失函数定义如下：

$$ \Theta_{D} = \arg\min_{\Theta_{D}} \sum_{i=1}^{N_{d}}\|CNN_{D}(\mathbf{I}_{i};\Theta_{D})-D_{i}\|_{1}^{2}$$

其中，$$\Theta_{D}$$ 是 CNN 的参数，$$N_{d}$$ 是训练图像的数量，CNN 第二个分支输出的特征图送入非刚性注册层。

##### 非刚性注册层（Non-rigid  Registration Layer）

作者设计了要给非刚性注册层来连接 CNN 和 RNN 的数据，这个层的输入有三个部分：估计出的人脸三维信息、CNN 输出的人脸深度图和 CNN 输出的特征图。非刚性注册层设计的目的是将 CNN 输出的特征图通过人脸三维点云和人脸深度图来正面化特征图，并去除特征图中的背景信息，保证送入 RNN 的特征图中人脸区域是一致的。该层结构如下图所示：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_11.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">非刚性注册层</div>
</center>

##### RNN 网络

RNN 模块是根据 $$N_{f}$$ 帧的输入序列 $$\{\mathbf{I}_{j}\}_{j=1}^{N_{f}}$$ 来估计 rPPG 信号 $$f$$。它包括一个 LSTM 层，一个全连接层和一个 FFT 层，FFT 层是将全连接层的响应特征向量转化为傅里叶域。RNN 损失函数定义如下：

$$ \Theta_{R} = \arg\min_{\Theta_{R}} \sum_{i=1}^{N_{s}} \|RNN_{R}([\{\mathbf{F}_{j}\}_{j=1}^{N_{f}}];\Theta_{R})-f_{i}\|_{1}^{2} $$

其中，$$\Theta_{R}$$ 是 RNN 的参数，$$\mathbf{F}_{j} \in \mathbb{R}^{32\times 32}$$ 是经过非刚性注册层后正面化后的特征图，$$N_{s}$$ 是序列的个数。


## Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment[^5]

论文中提出了一个称为姿态条件的树状卷积神经网络（Pose Conditioned Dendritic Convolution Neural Network, PCD-CNN），首先估计出人脸的 3D 姿态信息，将这个 3D 姿态信息作为关键点定位的一个条件，并且在要给分类网络后面又加入了第二个模块化的分类网络来提高关键点定位的精度。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_12.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">（a）图中卷积层上的虚线表示的是残差连接。姿态网络是树状关键点网络的一个条件，图中灰色框中的模型为作者提出的 PCD-CNN 框架，在蓝色框中的第二个分类网络模块作为一个辅助任务（遮挡、可见性），同时包括一个卷积-反卷积模块来精调关键点定位的记过。（b）人脸关键点的树状结构。</div>
</center>

#### 条件 3D 姿态模型

作者使用概率模型来建模输入图像 $$\mathbf{I} \in \mathbb{R}^{w\times h\times 3}$$，3D 姿态 $$\mathbf{P}\in \mathbb{R}^{3}$$，2D 关键点 $$\mathbf{C}\in \mathbb{R}^{N\times 2}$$，其中 $$N$$ 是关键点的数量，联合概率和条件概率可以分别表示如下：

$$ p(\mathbf{C}, \mathbf{P}, \mathbf{I}) = p(\mathbf{C}|\mathbf{P},\mathbf{I})p(\mathbf{P}|\mathbf{I})p(\mathbf{I}) $$

$$ p(\mathbf{C},\mathbf{P}|\mathbf{I}) = \frac{p(\mathbf{C},\mathbf{P},\mathbf{I})}{p(\mathbf{I})} = \underbrace{p(\mathbf{P}|\mathbf{I})}_{CNN}\underbrace{p(\mathbf{C}|\mathbf{P}\mathbf{I})}_{PCD-CNN} $$

乘积的第一项用一个 CNN 模型来估计人脸图像的 3D 姿态，乘积的第二项用一个卷积网络和多个成树状结构的反卷积网络实现。

这篇论文没有看的太明白。
{: .notice}


## FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis[^6]

不同与 GAN 的两者博弈，作者提出了一个成为三者博弈（Three-Player GAN）的方法，除了 GAN 的生成模型判别模型，作者增加了一个身份判别的模型，来生成身份保持的合成人脸，该方法命名为 FaceID-GAN。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_13.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">FaceID-GAN 网络框架。G 是生成器，输入包括和身份有关的特征、随机噪声向量和形状有关的特征，输出为特定条件下的合成人脸图像。P 用于估计人脸的形状，C 用于身份判别并提供和身份有关的特征。D 为判别器。</div>
</center>

输入图像 $$x^{r}$$ 的大小为 $$128\times 128$$，P 基于现有的 3DMM 模型来估计出人脸的形状特征 $$f_{p}^{r} \in \mathbb{R}^{229}$$，$$f_{p}^{'} = g(f_{p}^{r})$$ 是所需要的表示人脸姿态和表情的特征。C 作为人脸识别模块用于提取身份相关的特征 $$f_{id}^{r}\in \mathbb{R}^{256}$$，G 将 $$f_{p}^{'}$$，$$f_{id}^{r}$$ 和 一个随机变量 $$\mathbf{z} \in \mathbb{R}^{128}$$ 作为输入，输出大小为 $$128\times 128$$ 的合成人脸图像 $$x^{s}$$。

三者博弈的损失函数定义如下：

$$ \min_{\Theta_{D}}\mathcal{L}_{D} = R(x^{r}) - k_{t}R(x^{s}) $$

$$ \min_{\Theta_{C}}\mathcal{L}_{C} = \phi(x^{r},l_{id}^{r}) + \lambda \phi(x^{s}, \mathcal{l}_{id}^{s}) $$

$$ \min_{\Theta_{G}}\mathcal{L}_{G} = \lambda_{1}R(x^{s}) + \lambda_{2}d^{cos}(f_{id}^{r}, f_{id}^{s}) + \lambda_{3}d^{l2}(f_{p}^{'}, f_{p}^{s}) $$

下面分别介绍各个部分。

#### 判别器 D

论文中的判别器并不是判别生成的人脸图像为真或者假，作者自动编码机（auto-encoder）作为判别器，目的是将合成的人脸图像作为输入来重构出输入图像并最小化输入图像和输出图像之间像素级的距离。所以有 

$$R(x) = \|x-D(x)\|_{1}$$

判别模型中的 $$k_{t}$$ 通过如下公式动态调整：

$$ k_{t+1} = k_{t} + \lambda_{k}(\gamma R(x^{r})-R(x^{s})) $$

#### 分类器 C

输入人脸图像 $$x^{r}$$ 和合成人脸图像 $$x^{s}$$ 的身份特征分别表示为 $$f_{id}^{r}$$ 和 $$f_{id}^{s}$$，分类器 C 将真实人脸图像和合成人脸图像分别作为两个类别，即 1-of-2N 问题，真实人脸图像的类别为前 N 个类别，合成人脸图像为后 N 个类别，使用交叉熵作为损失函数：

$$ \phi(x^{r},l_{id}^{r}) = \sum_{j}-\{l_{id}^{r}\}_{j}\log (\{C(x^{r})\}_{j}) $$

$$ \phi(x^{s},l_{id}^{s}) = \sum_{j}-\{l_{id}^{s}\}_{j}\log (\{C(x^{s})\}_{j}) $$

其中，$$j\in [1,2N]$$ 表示第 $$j$$ 个类别。论文中作者指出，将真实人脸和合成人脸作为 $$2N$$ 个类别是不合理的，所以引入了损失权重 $$\lambda$$ 来平衡合成人脸损失的贡献。

#### 生成器 G

除了最小化 $$R(x^{s})$$，同时最小化两个距离函数：

$$ d^{cos}(f_{id}^{r},f_{id}^{s}) = 1 - \frac{f_{id}^{r}f_{id}^{s}}{\|f_{id}^{r}\|_{2}\|f_{id}^{s}\|_{2}} $$

$$ d^{l2}(f_{p}^{'},f_{p}^{s}) = \|f_{p}^{s}-f_{p}^{'}\|_{2}^{2} $$

生成器中的损失函数用来最小化真实人脸图像和合成人脸图像的身份特征之间的余弦值，同时不管人脸姿态和表情如何，最小化两个人脸图像形状特征之间的欧式距离。

### 实验

作者基于论文中提出的 FaceID-GAN 模型，做了人脸正面化实验以及人脸指定角度的旋转实验，同时又做了姿态和表情的转换实验。使用消融实验来验证方法的有效性。基于合成人脸身份保持属性，针对合成的人脸做了人脸验证实验。

## Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes[^7]

论文中提出的一种人脸超分辨率生成的方法，作者指出，由低分辨率的人脸图像生成高分辨率的人脸图像是一个 one-to-many 的问题，该歧义性会导致生成的高分辨率人脸图像细节上的畸变和人脸属性的改变（比如性别的改变）。基于此，作者提出了使用带有其他人脸属性信息的补充残差图像或者特征图可以显著降低人脸超分辨率生成时的歧义性。基于此，作者提出了一个包括一个上采样网络和一个判别网络的属性嵌入上采样网络，上采样网络接受人脸属性向量用来生成超分辨率的人脸图像，判别网络用来判别生成的人脸是否具有设计的属性。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_14.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">属性嵌入的上采样网络。网络包含两部分：上采样网络和判别网络。上采样网络的输入是低分辨率的人脸图像和属性向量，判别网络的输入是真实的高分辨率人脸图像和生成的高分辨率人脸图像和指定的属性向量。</div>
</center>

低分辨率的人脸图像到高分辨率的人脸图像是要给 one-to-many 的问题，所以在做高分辨率人脸图像生成的时候，加入语义信息（比如人脸属性）可以减少这个歧义性。整个的网络包括两部分：上采样网络和判别网络。上采样网络用于将嵌入的人脸属性和低分辨率的人脸图像生成高分辨率的人脸图像，判别网络用于限制上采样网络生成的高分辨率人脸图像更接近真实的高分辨率人脸图像，且具有指定的人脸属性特征。

#### 属性嵌入的上采样网络

上采样网络包括一个属性嵌入的自动编码机和一系列上采样层。在论文中，作者提出了一个属性嵌入方法，即采用一个带有跨层连接的自动编码机来嵌入人脸属性，跨层连接可以将从低分辨率人脸图像提取的特征和属性特征结合起来。在自动编码机的中间层，作者将人脸图像特征和属性特征做了拼接操作来将属性特征嵌入网络中。

由于低分辨率的人脸图像很难做到人脸关键点的对齐，作者在网络中加入了空间转换网络（spatial transformer networks(STNs)）来补偿没有做人脸对齐带来的误差。

上采样网络的损失函数有两部分：像素级别的欧式距离损失（pixel-wise $l_{2}$ loss）和感知损失（perceptual loss），其目的是让生成的高分辨率人脸图像在视觉效果上更接近于真实的高分辨率人脸图像。

#### 判别网络

判别网络用来判别生成的高分辨率人脸图像是否包含指定的人脸属性特征以及约束生成的高分辨率人脸图像和真实的人脸图像尽可能的接近。如网络结构图中黄色箭头所示，属性向量经过复制后达到和判别模型中间层的特征图一样的大小后将其拼接起来。

## Exploring Disentangled Feature Representation Beyond Face Identification[^8]

论文提出了一个成为 identity Distilling and Dispelling Autoencoder 的框架，用来提取人脸的身份相关特征 $$f_{\mathcal{T}}$$ 和身份无关特征 $$f_{\mathcal{P}}$$。通过采用 two-stream 的结构，学习到的分离特征不仅可以表示人脸身份或者属性，还可以用来重构出输入人脸。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_15.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">身份相关和无关特征的自动编码模型。</div>
</center>

#### 学习分离的人脸特征（Learning Disentangled Face Features）

Distilling and Dispelling Autoencoder 框架是一个可以 end-to-end 的学习人脸分离特征的框架。给定一张人脸图像 $$x$$，经过共享的一系列卷积层 $$E_{\theta_{enc}}(x)$$ 之后引入了 two-stream 结构，分支 $$B_{\theta_{\mathcal{T}}}$$ 用来提取人脸身份相关的特征 $$f_{\mathcal{T}}\in \mathbb{R}^{N_{\mathcal{T}}}$$，分支 $$B_{\theta_{\mathcal{P}}}$$ 用来提取人脸身份无关的特征 $$f_{\mathcal{P}}\in \mathbb{R}^{N_{\mathcal{P}}}$$。$$f_{\mathcal{T}}$$ 和 $f_{\mathcal{T}}$$ 共同来通过 $$D_{\theta_{dec}}$$ 恢复出一张人脸图像。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_16.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">各个监督信号的回传。</div>
</center>

##### 身份相关特征的学习（Identity Distilling Branch）

$$f_{\mathcal{T}}$$ 是输入图像经过 $$E_{\theta_{enc}}(x)$$ 和 $$B_{\theta_{\mathcal{T}}}$$ 之后提取到的人脸身份相关特征，即：$$f_{\mathcal{T}}=B_{\theta_{\mathcal{T}}}(E_{\theta_{enc}}(x))$$。采用人脸分类任务，即经过非线性变化映射为人脸类别的概率分布，如下所示：

$$y_{\mathcal{T}}=softmax(\mathbf{W}_{\mathcal{T}}f_{\mathcal{T}}+b_{\mathcal{T}})$$

交叉熵损失定义如下：

$$\mathcal{L}_{\mathcal{I}}=\sum_{j=1}^{N_{ID}}-g_{\mathcal{I}}^{j}\log y_{\mathcal{T}}^{j} = -\log y_{\mathcal{T}}^{t}$$

式中，$$g_{\mathcal{I}}$$ 是人脸的真实标签，$$t$$ 是标签的下标。

##### 身份无关特征学习（Identity Dispelling Branch）

身份无关特征学习分支去除掉人脸身份相关的特征，试图学习到剩余的人脸信息特征。和身份相关学习分支类似，学习到特征 $$f_{\mathcal{P}}=B_{\theta_{\mathcal{P}}}(E_{\theta_{enc}}(x))$$，然后做人脸分类任务，即学习的人脸类别概率分布 $$y_{\mathcal{P}}=softmax(\mathbf{W}_{\mathcal{P}}f_{\mathcal{P}}+b_{\mathcal{P}})$$。采用交叉熵损失作为对抗监督信号 $$\mathcal{L}_{\mathcal{I}}^{adv}=-\log y_{\mathcal{P}}^{t}$$，不同与 $$\mathcal{L}_{\mathcal{T}}$$ 的损失在会回传到模型的输入层，$$\mathcal{L}_{\mathcal{I}}^{adv}$$ 只更新最后分类层的参数。

另外，论文中提出需要额外的学习监督信号来训练学习到的身份无关特征能够欺骗对抗监督信号 $$\mathcal{L}_{\mathcal{I}}^{adv}$$，这个监督信号使得映射出的类别概率分布尽可能的相等，即都能于 $$\frac{1}{N_{ID}}$$，该损失定义如下：

$$\mathcal{L}_{\mathcal{H}}=\sum_{j=1}^{N_{ID}}u_{\mathcal{I}}^{j}\log y_{\mathcal{P}}^{j}=\frac{1}{N_{ID}}\sum_{j=1}^{N_{ID}}\log y_{\mathcal{P}}^{j}$$

监督信号 $$\mathcal{L}_{\mathcal{H}}$$ 的损失回传到模型的输入层。

##### 编码-解码结构

论文中使用 $$l_{2}$$ 距离来度量人脸重构的损失

$$\mathcal{L}_{\mathcal{X}}=\frac{1}{2}\|x-D_{\theta_{dec}}(f_{\mathcal{T}},f_{\mathcal{P}})\|_{2}^{2}$$

在身份相关特征的学习分支，$$\mathcal{L}_{\mathcal{I}}$$ 使得 $$f_{\mathcal{T}}$$ 能够表示身份相关的特征，重构损失 $$\mathcal{L}_{\mathcal{X}}$$ 通过重构原始输入图像来约束 $$f_{\mathcal{P}}$$ 能够表示剩余的人脸身份无关的特征信息。

##### 随机增广（Statistical Augmentation）

在学习到的特征上增加一个高斯噪声

$$\tilde{f}_{\ell}^{i}=f_{\ell}^{i}+\epsilon \sigma_{\ell}^{i}, ~\forall i \in \{1,...,N_{\ell}\}~ with ~\epsilon \thicksim \mathcal{N}(0,1)$$

其中，$$\ell \in \{\mathcal{T}_{id},\mathcal{P}\}$$ 表示不同的类别，尺度值表示 $f_{\ell}$ 中每个元素的标准差。

##### 学习算法

模型最终的目标函数定义如下：

$$ \mathcal{L} = \lambda_{\mathcal{T}}\mathcal{L}_{\mathcal{I}} + \lambda_{\mathcal{P}}(\mathcal{L}_{\mathcal{I}}^{adv} + \mathcal{L}_{\mathcal{H}}) + \lambda_{\mathcal{X}}(\tilde{\mathcal{L}}_{\mathcal{X}}+\mathcal{L}_{\mathcal{X}}) $$

其中，$$\tilde{\mathcal{L}}_{\mathcal{X}}$$ 是经过增广后的特征重构损失。

### 实验

论文中作者分别做了人脸识别与验证、人脸属性识别和人脸编辑实验。

#### 人脸识别与验证

作者分别对特征 $$f_{\mathcal{T}}$$ 和 $$f_{\mathcal{P}}$$ 计算余弦相似度，从实验汇报上来看，在 LFW 数据集上，特征 $$f_{\mathcal{T}}$$ 的人脸识别和验证率分别为 99.78% 和 99.63%，远远高于特征 $$f_{\mathcal{P}}$$ 的结果（64.13% 和 5.3%），由此可见，$$f_{\mathcal{T}}$$ 比 $$f_{\mathcal{P}}$$ 更加的具有可判别行，即 $$f_{\mathcal{T}}$$ 可表示人脸身份相关特征，而 $$f_{\mathcal{P}}$$ 表示的是人脸身份无关的特征。

#### 人脸属性识别

作者训练一个线性 SVM 来对特征 $$f_{\mathcal{T}}$$ 和 $$f_{\mathcal{P}}$$ 做人脸属性识别，识别精度分别为 79.78% 和 81.99%，之后作者对人脸的 40 个属性分别进行了分析，将这些属性划分为两类：身份相关的属性和身份无关的属性。

#### 人脸编辑

作者结合不同的  $$f_{\mathcal{T}}$$ 和 $$f_{\mathcal{P}}$$ 之后经过 $$D_{\theta_{dec}}$$ 后实现对人脸身份和属性的编辑，比如对人脸单个属性的编辑操作：

$$f_{\mathcal{P}}^{*}=f_{\mathcal{P}}+\alpha_{n}\mathbf{w}_{n}$$

其中，$$\mathbf{w}_{n}$$ 是由训练 SVM 得到的属性之间的最大边界向量。$$\alpha_{n}$$ 是一个尺度值。当然，也可以一次编辑多个特征：$$f_{\mathcal{P}}^{*}=f_{\mathcal{P}}+\sum_{n=1}^{N_{att}}\alpha_{n}\mathbf{w}_{n}$$。对人脸身份的编辑是将当前身份下的人脸的 $$f_{\mathcal{T}}$$ 直接替换为其他身份下人脸的 $$f_{\mathcal{T}}$$。当然，也可以做一个平滑变换：$$f_{\mathcal{T}}^{*}=\beta f_{\mathcal{T}}^{A} + (1-\beta) f_{\mathcal{T}}^{B}$$。

## Look at Boundary: A Boundary-Aware Face Alignment Algorithm[^9]

论文提出了一种通过估计人脸边界（包括五官的边界）来减少关键点定位中点的位置摸棱两可的问题。文中解释了为什么要使用边界，怎么样使用边界一起做边界估计和关键点定位之间的关系三个问题。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_17.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">边界相关的人脸关键点定位框架。（a）边界特征图回归器。（b）人脸图像和回归出的边界特征图用于最终的人脸关键点定位。（c）边界特征图的真假判别器。</div>
</center>

#### 构造边界特征图的回归目标

给定一张人脸图像 $$I$$，标注出的人脸关键点集合为 $$S={s_{l}}_{l=1}^{L}$$。$$K$$ 个子集 $$S_{i}\in S$$ 分别代表了 $$K$$ 个边界线上的关键点集合。对于每一个边界线集合，$$S_{i}$$ 通过插值的方式形成稠密的边界线，通过设置边界线上的点的数值为1，其他位置的点为0，即每条边界线可得到和人脸图像 $$I$$ 尺寸一样的一个二值边界图 $$B_{i}$$。通过距离转换和高斯表达式，将每条边界线对应的二值边界线图转换为边界特征图 $$M_{i}$$。公式见论文。

#### 边界特征图回归器

使用 MSE (Mean square error) 来优化生成的边界特征图和真实特征图的误差，但这样受遮挡的影响比较大，由此论文中作者引入了 MPL (message passing layers) 用来不同边界线之间传递信息。单一使用 MSE 会导致生成的特征图的质量看起来偏向于模糊和不真实，所以作者使用了一个判别器来判别生成的特征图和真实特征图的真假，依次来约束生成的样本分布和真实样本分布的一致性。

## Towards Pose Invariant Face Recognition in the Wild[^10]

人脸姿态的变化在人脸识别任务中是一个关键的挑战，目前的解决方法要么是提取人脸姿态无关的特征用于人脸识别，要么是在提取人脸特征之前将侧面人脸转正。论文中作者将这两种方法结合，提出了一种姿态无关模型（Pose Invariant Model, PIM）来做自然场景下的人脸识别。作者指出该方法有三个创新点。第一，PIM 包含了一个人脸正向子网络（Face Frontalization sub-Net, FFN）和要给判别学习子网络（Discriminative Learning sub-Net, DLN）。第二，FFN 是一个双路径的判别生成网络，同时感知全局结构信息和局部细节信息。第三，DLN 是一个通用的卷积神经网络，用来学习可判别的特征表示。

### 方法

#### 人脸正向子网络
人脸正向子网络包含有两个路径的生成网络，全局路径生成器 $$G_{\theta^{g}}$$ 有一个下采样的编码器 $$G_{\theta_{E}^{g}}$$ 和一个上采样的解码器 $$G_{\theta_{D}^{g}}$$，局部路径生成器 $$G_{\theta^{l}}$$ 也是一个自动编码机结构，有四个独立的子网络来分别学习正向四个中心裁切出的人脸局部子图：左眼、右眼、鼻子和嘴。为了增加生成模型对其他因素的鲁棒性，作者在模型的中间层加入了高斯噪声 $$z$$。

形式上，用 $$I_{tr}$$ 表示已给定关键点标注的人脸侧面图像，生成的正面人脸为 $$I^{'}=G_{\theta}(I_{tr})$$。要学习的参数为 $${\theta^{g},\theta_{i}^{l}}$$，优化目标如下：

$$\mathcal{L}_{G_{\theta}}=-\mathcal{L}_{adv} + \lambda_{0}\mathcal{L}_{ece} \lambda_{1}\mathcal{L}_{domain} + \lambda_{2}\mathcal{L}_{pixel} + \lambda_{3}\mathcal{L}_{sym} + \lambda_{4}\mathcal{L}_{TV}$$

其中，$$\mathcal{L}_{adv}$$ 是对抗损失，用来增加生成人脸图像的真实性。$$\mathcal{L}_{ece}$$ 是一个交叉熵损失用来保持生成人脸的身份信息。$$\mathcal{L}_{domain}$$ 是领域对损失，来提高生成器生成不同领域的能力。$$\mathcal{L}_{pixel}$$ 是像素级别的 $$l_{1}$$ 损失，$$\mathcal{L}_{sym}$$ 是为了消除自遮挡问题的合成损失，$$\mathcal{L}_{TV}$$ 是为了减少生成图像中的钉状突起而设置的全部变量损失。$${\lambda_{k}}(k=0,1,2,3,4)$$ 是不同损失的权重参数。

这篇论文没有看的太明白。
{: .notice}

## FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors[^11]

作者指出，人脸的先验知识有利于人脸超分辨图像的生成。基于此，设计了一个人脸超分辨模型（Face Super-Resolution Network, FSRNet），该模型利用人脸的几何先验，比如人脸的关键点信息和语义信息等，来对非常低分辨率的人脸进行超分辨率处理。首先，作者构造一个粗糙的超分辨率网络来恢复出一个人脸的粗糙超分辨图像，然后粗糙的超分辨率图像分别送入两个分支：一个精细的超分辨率编码器网络和一个人脸先验估计网络。精细的超分辨编码器网络来提取人脸图像的特征，人脸先验估计网络用来估计人脸的关键点和语义信息。两个分支的结果一起送入一个精细超分辨解码器来生成最后的超分辨率人脸图像。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_18.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">FSRNet 网络结构。‘Conv’ 表示带有激活函数的卷积层，‘k3n64s1’ 表示卷积核的大小是 3x3，特征图的数量是 64，步长为 1。</div>
</center>

论文中提出的基础的 FSRNet 包含四部分：粗糙的超分辨网络、精细的超分辨编码器、人脸先验估计网络和精细的超分辨解码器。$$x$$ 是低分辨率的输入图像，$$y$$ 和 $$p$$ 分别是恢复出的超分辨率图像和估计出的先验信息。

由于低分辨率的人脸图像很难提取出先验信息，所以作者首先通过要给粗糙的超分辨网络来恢复出要给粗糙的超分辨图像：

$$y_{c} = \mathcal{C}(x)$$

其中，$$\mathcal{C}$$ 是低分辨率图像 $$x$$ 到粗超分辨率图像的映射。然后，$$y_{c}$$ 分别送入先验估计网络 $$\mathcal{P}$$ 和精细超分辨编码器 $$\mathcal{F}$$

$$p = \mathcal{P}(y_{c}), f = \mathcal{F}(y_{c})$$

其中，$$f$$ 是精细超分辨编码器提取出的特征图。精细超分辨解码器通过将图像特征图 $$f$$ 和先验信息 $$p$$ 拼接后用来恢复最终的超分辨图像

$$y = \mathcal{D}(f, p)$$

给定 N 个样本的训练集 $$\{x^{(i)},\tilde{y}^{(i)},\tilde{p}^{(i)}\}_{i=1}^{N}$$，其中 $$\tilde{y}^{(i)}$$ 是低分辨输入图像 $$x^{(i)}$$ 对应的超分辨图像，$$\tilde{p}^{(i)}$$ 是对应的真实先验信息，FSRNet 的损失函数定义如下：

$$\mathcal{L}_{F}(\Theta) = \frac{1}{2N}\sum_{i=1}^{N}\{\|\tilde{y}^{(i)} - y_{c}^{(i)}\|^{2} + \alpha\|\tilde{y}^{(i)} - y^{(i)}\|^{2} + \beta\|\tilde{p}^{(i)} - p^{(i)}\|^{2}\}$$

其中，$$\Theta$$ 是网络参数集合，$$\alpha$$ 和 $$\beta$$ 是粗糙超分辨损失和先验损失的权重。$$y^{(i)}$$ 和 $$p^{(i)}$$ 分别是最终恢复出的超分辨图像和估计出的先验信息。

#### FSRGAN

GAN 能比单一的使用 MSE 的模型产生更加真实的图像，作者在 FSRNet 的基础上加入了条件 GAN 模型

$$\mathcal{C}(F,C) = \mathbb{E}[\log C(\tilde{y}, x)] + \mathbb{E}[\log(1 - C(F(x), x))]$$

其中，$$C$$ 输出的是输入图像是真实图像的概率，$$\mathbb{E}$$ 是概率分布的期望。除了对抗损失 $$\mathcal{L}_{C}$$，作者还加入了一个在预训练 VGG-16 模型的高层特征图上定义感知损失（perceptual loss）来获取和感知相关的特性

$$\mathcal{L}_{P} = \|\phi(y)-\phi(\tilde{y})\|^{2}$$

其中，$$\phi$$ 是固定的预训练 VGG 模型，将输入图像 $$y$$ 和 $$\tilde{y}$$ 映射到特征空间。至此，FSRGAN 最终的目标函数如下：

$$\arg \min_{F} \max_{C} \mathcal{L}_{F}(\Theta) + \gamma_{C}\mathcal{L}_{C}(F,C) + \gamma_{P}\mathcal{L}_{P}$$

### 人脸超分辨处理中的先验知识

作者使用 Helen 数据集来验证先验信息对于人脸超分辨率处理是有效的，Helen 数据集有真实的 194 个关键点的位置信息和 11 个人脸语义图。

对比实验如下图所示：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_19.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">验证人脸先验信息的有效性。（a）不使用人脸先验模型和使用真实人脸先验模型的人脸超分辨率结果对比，（b）使用不用数量关键点的人脸超分辨结果对比，（c）使用不同类型的语义图的结果。</div>
</center>

<kbd>完</kbd>

[^1]: Y.C. Bai, Y.Q. Zhang, M.L. Ding and B. Ghanem, Finding Tiny Faces in the Wild with Generative Adversarial Network, In Proc. CVPR, 2018.
[^2]: H.Y. Yang, D. Huang, Y.H. Wang and A.K. Jain, Learning Face Age Progression: A Pyramid Architecture of GANs, In Proc. CVPR, 2018.
[^3]: A. Bulat and G. Tzimiropoulos, Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs, In Proc. CVPR, 2018.
[^4]: Y.J. Liu, A. Jourabloo and X.M. Liu, Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision, In Proc. CVPR, 2018.
[^5]: A. Kumar and R. Chellappa, Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment, In Proc. CVPR, 2018.
[^6]: Y.J. Shen, P. Luo, J.J. Yan, X.G. Wang and X.O. Tang, FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis, In Proc. CVPR, 2018.
[^7]: X. Yu, B. Fernando, R. Hartley and F. Porikli, Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes, In Proc. CVPR, 2018.
[^8]: Y. Liu, F.Y. Wei, J. Shao, L. Sheng, J.J. Yan and X.G. Wang, Exploring Disentangled Feature Representation Beyond Face Identification, In Proc. CVPR, 2018.
[^9]: W.Y. Wu, C. Qian, S. Yang, Q. Wang, Y.C. Cai and Q. Zhou, Look at Boundary: A Boundary-Aware Face Alignment Algorithm, In Proc. CVPR, 2018.
[^10]: J. Zhao, Y. Cheng, Y. Xu, L. Xiong, J.S. Li, F. Zhao, Towards Pose Invariant Face Recognition in the Wild, In Proc. CVPR, 2018.
[^11]: Y. Chen, Y. Tai, X.M. Liu, C.H. Shen and J. Yang, FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors, In Proc. CVPR, 2018.
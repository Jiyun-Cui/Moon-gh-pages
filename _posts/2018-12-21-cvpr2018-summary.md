---
layout: post
title:  "CVPR2018 Summary"
date:   2018-12-21
excerpt: "Just about everything you'll need to style in the theme: headings, paragraphs, blockquotes, tables, code blocks, and more."
tag:
- CVPR2018 
- GAN
- Face Detection
- Face Age Progression
- Face Anti-Spoofing
- Face Alignment
- Face Synthesis
- Face Super-Resolving
comments: true
---

## Finding Tiny Faces in the Wild with Generative Adversarial Network[^1]

该论文提出的方法主要解决的问题是图像中小人脸的检测问题。针对检测模型提取出的Proposal，该论文基于 GAN 的方法将其中分辨率小且模糊的人脸进行超分辨率及清晰化处理。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="http://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">论文提出的模型流程图：(A) 送入网络的输入图像；(B)  MB-FCN 检测器，可裁切出输入图像中的人脸区域（正例图像）和非人脸区域（反例图像），这些数据用来送入后面的生成器和判别器用于模型训练；(C) 使用 MB-FCN 检测器生成的人脸区域数据（正例图像）和非人脸区域数据（反例图像）；(D) 提出的生成器网络结构，用于从一个低分辨率的图像重构出一个清晰的超分辨率图像（分辨率提高4倍），生成器网络包括上采样子网络和精调子网络；(D) 使用 VGG19 结构作为判别器，后面接有两个并行的 FC 层，分别用来判别输入的数据是否是真实的超分辨率图像和是否是人脸区域。</div>
</center>

#### GAN

常规的 GAN 的目标函数定义如下：

$$ \mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x\sim p_{data}(x)}[log D_{\theta}(x)] + \mathbb{E}_{z\sim p_{z}(z)}[log(1 - D_{\theta}(G_{\omega}(z)))] $$

其中，$$z$$ 表示随机噪声，$$x$$ 是真实的数据，$$\theta$$ 和 $$\omega$$ 分别是生成器 $$G$$ 和判别器 $$D$$ 的参数。模型优化的目标如下：

$$ \arg \min_{G}\max_{D} \mathcal{L}_{GAN}(G, D) $$

和常规的 GAN 的类似，除了输入的不是随机噪声，而是正例和反例图像，目标函数和优化目标如下：

$$ \arg \min_{\omega_{G}}\max_{\theta_{D}} \mathbb{E}_{(I^{HR},y)\sim p^{train}(I^{HR}, y)}[\log D_{\theta_{D}}(I^{HR}, y)] + \\
\mathbb{E}_{(I^{LR},y)\sim p^{G}(I^{LR}, y)}[\log (1 - D_{\theta_{D}}(G_{\omega_{G}}(I^{HR}, y)))] $$

其中，$$I^{LR}$$ 表示的是低分辨率的图像，$$I^{HR}$$ 是高分辨率的图像，$$y$$ 是输入图像的标签。在判别网络，判别器不仅要判别生成图像的高分辨率图像和真实的高分辨率图像，还要判别人脸图像和非人脸图像。

#### 损失函数

论文中采用了两种损失函数，分别是 pixel-wise loss 和 adversarial loss。


**pixel-wise loss.** 通过引入 pixel-wise MSE loss，可以让生成器生成的高分辨率图像和真实高分辨率图像在纹理信息上接近。损失函数如下：

$$ L_{MSE}(\omega) = \frac{1}{N} \sum_{i=1}^{N}(\| G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2} + \| G_{2_{\omega 2}}(G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2}) $$

其中，$$ I^{LR} $$ 和 $$ I^{HR} $$ 分别表示低分辨率模糊图像和高分辨率图像，$$ G_{1} $$ 代表了生成器网络中的上采样子网络，而 $$G_{2}$$ 代表了生成器网络中的精调子网络，$$\omega$$ 是生成器的参数。但 MSE 优化问题总是会丢失图像的高频信息，造成生成的图像偏向于模糊或者过平滑。

**Adversarial loss.** 为了得到更加真实清晰的细节信息，作者引入了 adversarial loss，定义如下：

$$ L_{adv} = \frac{1}{N}\sum_{i=1}^{N}\log (1 - D_{\theta}(G_{\omega}(I_{i}^{LR}))) $$

引入的 adversarial loss 使得生成器生成出来的图像更加的清晰，包含有更多的高频信息以可以欺骗判别器模型。

**Classification loss.** 训练数据除了有低分辨率和高分辨率之分，还有人脸和非人脸的差异，由此引入了 classification loss，用来判别是否是人脸图像。损失函数定义如下：

$$ L_{cls} = \frac{1}{N}\sum_{i=1}^{N}(\log (y_{i} - D_{\theta}(G_{\omega}(I_{i}^{LR}))) + \log (y_{i} - D_{\theta}(I_{i}^{HR}))) $$

其中 $$y_{i}$$ 是图像是否是人脸图像的标签，$$y_{i}=1$$ 表示图像是人脸图像，$$y_{i}=0$$ 表示的是非人脸图像。

**Objective funciton.** 目标函数是上述三个损失函数的加权求和：

$$ \max_{\theta}\min_{\omega} \frac{1}{N}\sum_{i=1}^{N}\alpha(\log (1 - D_{\theta}(G_{\omega}(I_{i}^{LR})) +  \log D_{\theta}(I_{i}^{HR})) + \\ 
(\| G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2} + \| G_{2_{\omega 2}}(G_{1_{\omega 1}}(I_{i}^{LR}) - I_{i}^{HR} \|^{2}) + \\ 
\beta(\log (y_{i} - D_{\theta}(G_{\omega}(I_{i}^{LR}))) + \log (y_{i} - D_{\theta}(I_{i}^{HR})))$$

其中，$$\alpha$$ 和 $$\beta$$ 分别代表权重。

### 消融实验

实验用的数据集是 WIDER FACE dataset，总共包含 32,203 张密集的人脸图像，其中按照人脸的大小，分为简单/中等/难三个子数据集。论文中作者做了消融实验来验证提出的一系列损失函数的有效性，如下图所示：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="http://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">消融实验：Baseline 方法和分别加入精调网络、对抗损失、分类损失和论文提出的方法在 WIDER FACE dataset 上的表现。</div>
</center>

 
## Learning Face Age Progression: A Pyramid Architecture of GANs[^2]

该论文提出了一个基于 GAN 的做年龄老化的方法，同时能够保持人的身份信息不改变。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_3.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">提出的年龄老化框架图。一个基于 CNN 的生成器 G 用于做年龄转换，欧式空间下的平方损失用来约束生成的图像。GAN 损失在图像的纹理信息到高层的语义信息上来判别生成的老化人脸和真实的老年人脸。身份保持损失在人脸特征空间约束人脸的身份信息。</div>
</center>

#### 判别器

和以往判别器采用对数似然作为目标函数不同，论文中使用最小均方损失代替对数似然。作为判别器的输入，正例为真实的年老人脸图像，而反例有两类，一个是真实的年轻人脸图像，另外一个是有生成器生成的年老人脸图像。论文中 GAN 的损失函数定义如下：

$$ \mathcal{L}_{GAN\_D} = \frac{1}{2}\mathbb{E}_{x\sim P_{old}(x)}[(D_{\omega}(\phi_{age}(x))-1)^{2}] + \\
\frac{1}{2}\mathbb{E}_{x\sim P_{young}(x)}[D_{\omega}(\phi_{age}(G(x)))^{2} + D_{\omega}(\phi_{age}(x))^{2}] $$


$$ \mathcal{L}_{GAN\_G} = \mathbb{E}_{x\sim P_{young}(x)}[D_{\omega}(\phi_{age}(G(x)) - 1)^{2}] $$

介于生成器 G 和判别器 D 之间有一个函数 $$\phi_{age}$$ 用来提取和年龄相关的特征。作者认为在不同年龄群体中的人脸会有一些共性和相似的纹理特征，所以提出了独立于判别器的特征提取函数 $$\phi_{age}$$ 用来提取和年龄相关的特征依次来更好的区分真实的年老人脸图像和生成的年老人脸图像。作者针对 $$\phi_{age}$$ 做了特殊的设计，首先使用 VGG-16 结构训练年龄分类任务的模型作为预训练，去除全连接层后，从输入图像的像素级别一直到高层的年龄相关的语义信息层来提取年龄相关的特征，即采用了一个金字塔的结构。

#### 身份保持

年龄老化之后的图像不应该改变人的身份信息，为此作者引入了年龄保持损失。用一个在大规模人脸数据集上训练做人脸分类任务的模型作为预训练模型，然后去除最后的分类层，直接用一个 triplet-loss 来做度量学习，在特征空间上约束人脸的特征距离。目标损失函数定义如下：

$$ \mathcal{L}_{identity} = \mathbb{E}_{x\sim P_{young}(x)}d(\phi_{id}(x), \phi_{id}(G(x))) $$

其中，$$\phi_{id}$$ 是预训练的人脸身份特征提取模型，$$d$$ 是人脸空间的平方欧式距离。

#### 目标函数

除了判别器损失和人脸保持损失i，作者又引入了一个 pixel-wise L2 损失函数来约束图像空间中生成的年老人脸图像和真实的年老人脸图像的距离，定义如下：

$$ \mathcal{L}_{pixel} = \frac{1}{W \times H \times C}\|G(x)-x\|_{2}^{2} $$

其中，$$x$$ 是输入的人脸图像，$$W$$、$$H$$ 和 $$C$$ 是图像的形状。

最终整个模型的训练损失如下：

$$ \mathcal{L}_{G} = \lambda_{\alpha}\mathcal{L}_{GAN\_G} + \lambda_{p}\mathcal{L}_{pixel} + \lambda_{i}\mathcal{L}_{identity} $$

$$ \mathcal{L}_{D} = \mathcal{L}_{GAN\_D} $$

### 实验

论文在两个数据集 MORPH 和 CACD 上做了大量的实验，包括人脸老化的定性分析和定量分析以及年龄老化后对人脸识别的影响。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_4.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">年龄老化的定性实验，前两行是在 CACD 数据集上的结果，后两行是在 MORPH 数据集上的结果。每组第一张图像是模型输入的年轻人脸图像，紧跟其后的三个人脸图像是做的老化到不同年龄阶段的效果。</div>
</center>

## Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs[^3]

论文提出了通过 GAN 方法来对小的人脸图像做超分辨率处理，进而来提高低分辨率下的人脸关键点定位的准确度。作者提出的模型成为 Super-FAN，能够同时进行好分辨率处理和人脸关键点定位任务。该模型包含了一个基于 GAN 的人脸超分辨率网络和一个通过热度图回归（heatmap regression）的人脸关键点定位的子网络。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_5.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Super-FAN 网络框架，包含三个模块：第一个是作者提出的人脸超分辨率生成网络，第二个是基于 WGAN 的判别器，用来判别原始的高分辨率人脸图像和生成的高分辨率人脸图像，第三个是人脸对齐网络，用来做生成的高分辨率人脸图像的关键点点定位任务并通过热图损失（heatmap loss）来提高超分辨的效果。</div>
</center>

#### 超分辨率网络（Super-resolution network）

作者提出了一个新的基于残差（residual-based）的网络结构用于超分辨率处理，网络的输入和输出的尺寸分别是 $$16\times 16$$ 和 $$64\times 64$$。类比于 SR-ResNet 包含有 15-1-1 的块结构，作者做了改进，提出了一个 12-3-2 的块结构，做出这个改进的依据是模型的最终目的是做超分辨处理，模型最后单个块（Block）不能充分生成清晰的细节信息。同时，作者去除了 SR-ResNet 中的一个长连接（long skip connection）。作者提出的生成器和 SR-ResNet 的网络结构图如下图所示：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_6.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">作者提出的基于残差的生成器和 SR-ResNet 模型对比</div>
</center>

##### 像素和感知损失（Pixel and perceptual losses）

**Pixel loss.** 作者使用像素级别的平均绝对误差（pixel-wise MSE loss）来最小化生成的高分辨率人脸图像和对应的真实高分辨率人脸图像之间的差异。定义如下：

$$ l_{pixel} = \frac{1}{r^{2}WH}\sum_{x=1}^{rW}\sum_{y=1}^{rH}(I_{x,y}^{HR}-G_{\theta_{G}}(I^{LR})_{x,y})^{2} $$

其中，$$W$$ 和 $$H$$ 表示低分辨率人脸图像 $$I^{LR}$$ 的宽和高，$$r$$ 是超分辨处理的上采样倍数。

**Perceptual loss.** 像素级别的平均绝对误差能够达到很高的 PSNR 评价指标，但也会缺失人脸图像的细节，真实度也很低。由此作者引入了感知损失，在 ResNet50（作者使用ResNet50作为生成器模型）的 B1、B2 和 B3 blocks 加入了感知损失，对于第 $$i$$ 层的特征的感知损失定义如下：

$$ l_{feature/i} = \frac{1}{W_{i}H_{i}}\sum_{x=1}^{W_{i}}\sum_{y=1}^{H_{i}}(\phi_{i}(I^{HR}))_{x, y} - \phi_{i}(G_{\theta_{G}}(I^{LR}))_{x,y})^{2} $$

其中，$$\phi_{i}$$ 表示第 $$i$$ 个块的最后一个的卷积层的特征图，$$W_{i}$$ 和 $$H_{i}$$ 是其尺寸。


#### 对抗网路（Adversarial network）

作者使用 WGAN 中的损失函数，定义如下：

$$ l_{WGAN} = \underset{\hat I\sim \mathbb{P}_{g}}{\mathbb{E}}[D(\hat I)] - \underset{I\sim \mathbb{P}_{r}}{\mathbb{E}}[D(I^{HR})] + \lambda \underset{\hat I\sim \mathbb{P}_{\hat I}}{\mathbb{E}}[(\|\nabla_{\hat I}D(\hat I) \|_{2}-1)^{2}] $$

#### 人脸对齐网络（Face Alignment Network）

前面提出的像素、感知和对抗损失只能恢复出清晰的人脸纹理信息，但并不能保证生成的高分辨率人脸的结构和低分辨率的人脸结构一致。为了解决这个问题，作者提出了一个通过热度图回归来做人脸关键点定位的网络，将这个网络和超分辨率处理模型进行整合，提出了一个热度图损失（heatmap loss）来约束生成的高分辨率人脸的结构信息。

论文中指出只要保证生成的高分辨率人脸图像的关键点热图（heatamp）和对应的真实高分辨率人脸图的关键点热图保持一致即可。所以作者并没有直接做人脸关键点定位操作，而是一个了两个 FAN 模型，一个和超分辨处理模块整合为一个端到端的模型，用于提取生成的高分辨率人脸图像的关键点热图，另外一个 FAN 在人脸关键点定位任务上做预训练，预训练的模型用来提取真实的高分辨率人脸图像中的关键点的热图。然后让这前一组热图尽可能的后一组热图保持一致。热图损失（heatmap loss）定义如下：

$$ l_{heatmap} = \frac{1}{N} \sum_{i=1}^{N}\sum_{ij} (\widetilde{M}_{i,j}^{n}-\widehat{M}_{i,j}^{n})^{2} $$

其中，$$ \widetilde{M}_{i,j}^{n}$$ 是由整合进超分辨率处理模块的 FAN 模块产生的合成的高分辨率人脸图像 $$\hat{I}_{HR}$$ 的第 $$n$$ 个关键点在像素坐标 $$(i, j)$$ 处的热图，$$\widehat{M}_{i,j}^{n}$$ 是有另外一个 FAN 生成的真实的高分辨率人脸图像 $$I_{HR}$$ 的热图。

#### 整个的训练损失（Overall training loss）

Super-FAN 的整个的训练损失定义如下：

$$ l^{SR} = \alpha l_{pixel} + \beta l_{feature} + \gamma l_{heatmap} + \zeta l_{WGAN} $$

其中，$$\alpha$$，$$\beta$$，$$\gamma$$ 和 $$\zeta$$ 是对应的权重。

### 实验

作者分别做了人脸超分辨率生成实验和针对生成的人脸图像做的人脸关键点定位实验。在人脸超分辨率生成实验中有定性和定量两种分析，定性的实验结果如下图所示，实验结果度量采用的是 PSNR 和 SSIM：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_7.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">在 LS3D-W balanced dataset 上基于 PSNR 和 SSIM 做的人脸超分辨率定性实验</div>
</center>

在人脸关键点定位实验中，作者首先在一个高分辨率的人脸数据集上训练人脸关键点定位模型 FAN，然后用这个模型来对生成的人脸高分辨率图像做关键点定位，定位的精度即作为人脸超分辨结果的精度。作者对人脸划分了不同的角度范围，实验结果如下：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_8.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">在 LS3D-W balanced test set 上的 AUC 结果</div>
</center>


## Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision[^4]

和以往人脸欺骗检测方法对输入人脸图像直接做二分类不同的是，作者利用了辅助的监督信号来指导人脸欺骗检测模型学习过程，提出了一个基于 CNN-RNN 模型来估计人脸像素级别的深度数据和估计 rPPG 信号，然后估计出的人脸深度和 rPPG 融合后来判别人脸图像是真实人脸还是欺骗人脸。


### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_9.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">基于 CNN-RNN 的人脸欺骗检测方法</div>
</center>

CNN 模块用人脸深度图作为监督信号来发掘真实人脸图像和人脸欺骗图像中细微的纹理特性，然后估计出的人脸深度图像和特征图送入一个非刚性注册层来生成对齐的特征图。RNN 模块用生成的对齐特征图作为输入，rPPG 信号作为监督，来获取视频帧中的时序变化。

#### 深度图像监督（Depth  Map Supervision）

真实人脸图像和欺骗人脸图像在深度图像上有本质的差异（真实人脸图像的深度范围在0到1，而欺骗人脸图像的深度值是0）。如何来获取人脸深度监督信息，论文中使用人脸三维重构的方法先重构出人脸的三维点云，然后通过人脸三维点云映射到二维的深度图像上，最终得到了真实人脸对应的尺寸为 $$32\times 32$$ 的人脸深度图像。

#### rPPG 监督信号（rPPG Supervision）

rPPG 信号提供了人脸纹理变化的时序信息，主要捕获血液流到在脸部产生的亮度变化。作者认为同一个人的心率变化是固定的，所以选择一个人在可控环境下的一段视频提取出的 rPPG 信号作为这个人其他 PIE 条件下的 rPPG 监督监督信号。对于欺骗人脸数据，作者在实验中将其 rPPG 信号设置为0。

#### 网络结构

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_10.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">论文突出的基于 CNN-RNN 的网络结构。每一层网络的卷积核的数量标在了每一层的上面，所有的卷积和的大小为3x3，步长为1，池化层的步长为2。颜色表示：橘色=卷积层，绿色=池化层，紫色=响应图。</div>
</center>

##### CNN 网络

CNN 网络设计为全卷积网络，网络的输入图像的大小为$$256\times 256$$，输出大小为$$32\times 32$$，输出有两个分支，一个是估计出的人脸深度图，另外一个是单通道的特征图。
监督损失函数定义如下：

$$ \Theta_{D} = \arg\min_{\Theta_{D}} \sum_{i=1}^{N_{d}}\|CNN_{D}(\mathbf{I}_{i};\Theta_{D})-D_{i}\|_{1}^{2}$$

其中，$$\Theta_{D}$$ 是 CNN 的参数，$$N_{d}$$ 是训练图像的数量，CNN 第二个分支输出的特征图送入非刚性注册层。

##### 非刚性注册层（Non-rigid  Registration Layer）

作者设计了要给非刚性注册层来连接 CNN 和 RNN 的数据，这个层的输入有三个部分：估计出的人脸三维信息、CNN 输出的人脸深度图和 CNN 输出的特征图。非刚性注册层设计的目的是将 CNN 输出的特征图通过人脸三维点云和人脸深度图来正面化特征图，并去除特征图中的背景信息，保证送入 RNN 的特征图中人脸区域是一致的。该层结构如下图所示：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_11.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">非刚性注册层</div>
</center>

##### RNN 网络

RNN 模块是根据 $$N_{f}$$ 帧的输入序列 $$\{\mathbf{I}_{j}\}_{j=1}^{N_{f}}$$ 来估计 rPPG 信号 $$f$$。它包括一个 LSTM 层，一个全连接层和一个 FFT 层，FFT 层是将全连接层的响应特征向量转化为傅里叶域。RNN 损失函数定义如下：

$$ \Theta_{R} = \arg\min_{\Theta_{R}} \sum_{i=1}^{N_{s}} \|RNN_{R}([\{\mathbf{F}_{j}\}_{j=1}^{N_{f}}];\Theta_{R})-f_{i}\|_{1}^{2} $$

其中，$$\Theta_{R}$$ 是 RNN 的参数，$$\mathbf{F}_{j} \in \mathbb{R}^{32\times 32}$$ 是经过非刚性注册层后正面化后的特征图，$$N_{s}$$ 是序列的个数。


## Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment[^5]

论文中提出了一个称为姿态条件的树状卷积神经网络（Pose Conditioned Dendritic Convolution Neural Network, PCD-CNN），首先估计出人脸的 3D 姿态信息，将这个 3D 姿态信息作为关键点定位的一个条件，并且在要给分类网络后面又加入了第二个模块化的分类网络来提高关键点定位的精度。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_12.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">（a）图中卷积层上的虚线表示的是残差连接。姿态网络是树状关键点网络的一个条件，图中灰色框中的模型为作者提出的 PCD-CNN 框架，在蓝色框中的第二个分类网络模块作为一个辅助任务（遮挡、可见性），同时包括一个卷积-反卷积模块来精调关键点定位的记过。（b）人脸关键点的树状结构。</div>
</center>

#### 条件 3D 姿态模型

作者使用概率模型来建模输入图像 $$\mathbf{I} \in \mathbb{R}^{w\times h\times 3}$$，3D 姿态 $$\mathbf{P}\in \mathbb{R}^{3}$$，2D 关键点 $$\mathbf{C}\in \mathbb{R}^{N\times 2}$$，其中 $$N$$ 是关键点的数量，联合概率和条件概率可以分别表示如下：

$$ p(\mathbf{C}, \mathbf{P}, \mathbf{I}) = p(\mathbf{C}|\mathbf{P},\mathbf{I})p(\mathbf{P}|\mathbf{I})p(\mathbf{I}) $$

$$ p(\mathbf{C},\mathbf{P}|\mathbf{I}) = \frac{p(\mathbf{C},\mathbf{P},\mathbf{I})}{p(\mathbf{I})} = \underbrace{p(\mathbf{P}|\mathbf{I})}_{CNN}\underbrace{p(\mathbf{C}|\mathbf{P}\mathbf{I})}_{PCD-CNN} $$

乘积的第一项用一个 CNN 模型来估计人脸图像的 3D 姿态，乘积的第二项用一个卷积网络和多个成树状结构的反卷积网络实现。

这篇论文没有看的太明白。
{: .notice}


## FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis[^6]

不同与 GAN 的两者博弈，作者提出了一个成为三者博弈（Three-Player GAN）的方法，除了 GAN 的生成模型判别模型，作者增加了一个身份判别的模型，来生成身份保持的合成人脸，该方法命名为 FaceID-GAN。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_13.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">FaceID-GAN 网络框架。G 是生成器，输入包括和身份有关的特征、随机噪声向量和形状有关的特征，输出为特定条件下的合成人脸图像。P 用于估计人脸的形状，C 用于身份判别并提供和身份有关的特征。D 为判别器。</div>
</center>

输入图像 $$x^{r}$$ 的大小为 $$128\times 128$$，P 基于现有的 3DMM 模型来估计出人脸的形状特征 $$f_{p}^{r} \in \mathbb{R}^{229}$$，$$f_{p}^{'} = g(f_{p}^{r})$$ 是所需要的表示人脸姿态和表情的特征。C 作为人脸识别模块用于提取身份相关的特征 $$f_{id}^{r}\in \mathbb{R}^{256}$$，G 将 $$f_{p}^{'}$$，$$f_{id}^{r}$$ 和 一个随机变量 $$\mathbf{z} \in \mathbb{R}^{128}$$ 作为输入，输出大小为 $$128\times 128$$ 的合成人脸图像 $$x^{s}$$。

三者博弈的损失函数定义如下：

$$ \min_{\Theta_{D}}\mathcal{L}_{D} = R(x^{r}) - k_{t}R(x^{s}) $$

$$ \min_{\Theta_{C}}\mathcal{L}_{C} = \phi(x^{r},l_{id}^{r}) + \lambda \phi(x^{s}, \mathcal{l}_{id}^{s}) $$

$$ \min_{\Theta_{G}}\mathcal{L}_{G} = \lambda_{1}R(x^{s}) + \lambda_{2}d^{cos}(f_{id}^{r}, f_{id}^{s}) + \lambda_{3}d^{l2}(f_{p}^{'}, f_{p}^{s}) $$

下面分别介绍各个部分。

#### 判别器 D

论文中的判别器并不是判别生成的人脸图像为真或者假，作者自动编码机（auto-encoder）作为判别器，目的是将合成的人脸图像作为输入来重构出输入图像并最小化输入图像和输出图像之间像素级的距离。所以有 

$$R(x) = \|x-D(x)\|_{1}$$

判别模型中的 $$k_{t}$$ 通过如下公式动态调整：

$$ k_{t+1} = k_{t} + \lambda_{k}(\gamma R(x^{r})-R(x^{s})) $$

#### 分类器 C

输入人脸图像 $$x^{r}$$ 和合成人脸图像 $$x^{s}$$ 的身份特征分别表示为 $$f_{id}^{r}$$ 和 $$f_{id}^{s}$$，分类器 C 将真实人脸图像和合成人脸图像分别作为两个类别，即 1-of-2N 问题，真实人脸图像的类别为前 N 个类别，合成人脸图像为后 N 个类别，使用交叉熵作为损失函数：

$$ \phi(x^{r},l_{id}^{r}) = \sum_{j}-\{l_{id}^{r}\}_{j}\log (\{C(x^{r})\}_{j}) $$

$$ \phi(x^{s},l_{id}^{s}) = \sum_{j}-\{l_{id}^{s}\}_{j}\log (\{C(x^{s})\}_{j}) $$

其中，$$j\in [1,2N]$$ 表示第 $$j$$ 个类别。论文中作者指出，将真实人脸和合成人脸作为 $$2N$$ 个类别是不合理的，所以引入了损失权重 $$\lambda$$ 来平衡合成人脸损失的贡献。

#### 生成器 G

除了最小化 $$R(x^{s})$$，同时最小化两个距离函数：

$$ d^{cos}(f_{id}^{r},f_{id}^{s}) = 1 - \frac{f_{id}^{r}f_{id}^{s}}{\|f_{id}^{r}\|_{2}\|f_{id}^{s}\|_{2}} $$

$$ d^{l2}(f_{p}^{'},f_{p}^{s}) = \|f_{p}^{s}-f_{p}^{'}\|_{2}^{2} $$

生成器中的损失函数用来最小化真实人脸图像和合成人脸图像的身份特征之间的余弦值，同时不管人脸姿态和表情如何，最小化两个人脸图像形状特征之间的欧式距离。

### 实验

作者基于论文中提出的 FaceID-GAN 模型，做了人脸正面化实验以及人脸指定角度的旋转实验，同时又做了姿态和表情的转换实验。使用消融实验来验证方法的有效性。基于合成人脸身份保持属性，针对合成的人脸做了人脸验证实验。

## Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes[^7]

论文中提出的一种人脸超分辨率生成的方法，作者指出，由低分辨率的人脸图像生成高分辨率的人脸图像是一个 one-to-many 的问题，该歧义性会导致生成的高分辨率人脸图像细节上的畸变和人脸属性的改变（比如性别的改变）。基于此，作者提出了使用带有其他人脸属性信息的补充残差图像或者特征图可以显著降低人脸超分辨率生成时的歧义性。基于此，作者提出了一个包括一个上采样网络和一个判别网络的属性嵌入上采样网络，上采样网络接受人脸属性向量用来生成超分辨率的人脸图像，判别网络用来判别生成的人脸是否具有设计的属性。

### 方法

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://cuijyer-images.oss-cn-beijing.aliyuncs.com/20181221_cvpr2018_summary_14.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">属性嵌入的上采样网络。网络包含两部分：上采样网络和判别网络。上采样网络的输入是低分辨率的人脸图像和属性向量，判别网络的输入是真实的高分辨率人脸图像和生成的高分辨率人脸图像和指定的属性向量。</div>
</center>

低分辨率的人脸图像到高分辨率的人脸图像是要给 one-to-many 的问题，所以在做高分辨率人脸图像生成的时候，加入语义信息（比如人脸属性）可以减少这个歧义性。整个的网络包括两部分：上采样网络和判别网络。上采样网络用于将嵌入的人脸属性和低分辨率的人脸图像生成高分辨率的人脸图像，判别网络用于限制上采样网络生成的高分辨率人脸图像更接近真实的高分辨率人脸图像，且具有指定的人脸属性特征。

#### 属性嵌入的上采样网络

上采样网络包括一个属性嵌入的自动编码机和一系列上采样层。在论文中，作者提出了一个属性嵌入方法，即采用一个带有跨层连接的自动编码机来嵌入人脸属性，跨层连接可以将从低分辨率人脸图像提取的特征和属性特征结合起来。在自动编码机的中间层，作者将人脸图像特征和属性特征做了拼接操作来将属性特征嵌入网络中。

由于低分辨率的人脸图像很难做到人脸关键点的对齐，作者在网络中加入了空间转换网络（spatial transformer networks(STNs)）来补偿没有做人脸对齐带来的误差。

上采样网络的损失函数有两部分：像素级别的欧式距离损失（pixel-wise $l_{2}$ loss）和感知损失（perceptual loss），其目的是让生成的高分辨率人脸图像在视觉效果上更接近于真实的高分辨率人脸图像。

#### 判别网络

判别网络用来判别生成的高分辨率人脸图像是否包含指定的人脸属性特征以及约束生成的高分辨率人脸图像和真实的人脸图像尽可能的接近。如网络结构图中黄色箭头所示，属性向量经过复制后达到和判别模型中间层的特征图一样的大小后将其拼接起来。

## Exploring Disentangled Feature Representation Beyond Face Identification[^8]

<kbd>完</kbd>

[^1]: Y.C. Bai, Y.Q. Zhang, M.L. Ding and B. Ghanem, Finding Tiny Faces in the Wild with Generative Adversarial Network, In Proc. CVPR, 2018.
[^2]: H.Y. Yang, D. Huang, Y.H. Wang and A.K. Jain, Learning Face Age Progression: A Pyramid Architecture of GANs, In Proc. CVPR, 2018.
[^3]: A. Bulat and G. Tzimiropoulos, Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs, In Proc. CVPR, 2018.
[^4]: Y.J. Liu, A. Jourabloo and X.M. Liu, Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision, In Proc. CVPR, 2018.
[^5]: A. Kumar and R. Chellappa, Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment, In Proc. CVPR, 2018.
[^6]: Y.J. Shen, P. Luo, J.J. Yan, X.G. Wang and X.O. Tang, FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis, In Proc. CVPR, 2018.
[^7]: X. Yu, B. Fernando, R. Hartley and F. Porikli, Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes, In Proc. CVPR, 2018.
[^8]: Y. Liu, F.Y. Wei, J. Shao, L. Sheng, J.J. Yan and X.G. Wang, Exploring Disentangled Feature Representation Beyond Face Identification, In Proc. CVPR, 2018.